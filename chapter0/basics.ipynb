{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65a1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m189.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1c1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de47034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(func:Callable[[np.ndarray],np.ndarray],inputparam:np.ndarray,delta:float=0.001)->np.ndarray:\n",
    "  return (func(inputparam+delta)-func(inputparam-delta)/2*delta)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f520007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(inputparam:np.ndarray)->np.ndarray:\n",
    "  return inputparam**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b52dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.001502,  4.002003,  9.001504, 16.000005, 24.997506, 35.994007])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative(f,np.array([1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable \n",
    "from typing import List\n",
    "Array_Function = Callable[[np.ndarray],np.ndarray]\n",
    "Chain = List[Array_Function]\n",
    "\n",
    "def chain_length_2(chain:Chain,a:np.ndarray)->ndarray:\n",
    "  assert len(chain)==2, \\\n",
    "  \"Length of input 'chain' should be 2\"\n",
    "\n",
    "  f1 = chain[0]\n",
    "  f2 = chain[1]\n",
    "\n",
    "  return f2(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "\n",
    "Array_Function = Callable([[np.ndarray],np.ndarray])\n",
    "Chain = List[Array_Function]\n",
    "\n",
    "def chain_length2(chain:Chain):\n",
    "  assert len(chain)==2, \\\n",
    "  \"the chain length should be of size 2\"\n",
    "\n",
    "  f1=chain[0]\n",
    "  f2=chain[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092b567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x:np.ndarray)->np.ndarray:\n",
    "  return 1/1-np.exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_deriv_2(chain:Chain,input_range:np.ndarray)->np.ndarray:\n",
    "  assert len(chain)==2, \\\n",
    "  \"this function requires 'Chain' objects fo length 2\"\n",
    "\n",
    "  assert input_range.ndim ==1,\\\n",
    "  \"function requires a one dimensional ndarray as input_range\"\n",
    "\n",
    "  f1=chain[0]\n",
    "  f2=chain[1]\n",
    "\n",
    "  f1_of_x = f1(input_range)\n",
    "  #df1/du(f1(x))\n",
    "  df1dx=deriv(f1,input_range)\n",
    "  # df2/du(f1(x))\n",
    "  df2du = deriv(f2,f1(input_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e2b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the dimesnion of input variables should be 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f2_output\n\u001b[32m     31\u001b[39m mychain = [square,sigmoid]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mderivativechain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmychain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mderivativechain\u001b[39m\u001b[34m(chain, input_variables)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mderivativechain\u001b[39m(chain:Chain,input_variables:np.ndarray)->np.ndarray:\n\u001b[32m     18\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chain)==\u001b[32m2\u001b[39m , \\\n\u001b[32m     19\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mthe length of chain should be 2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m np.ndim(input_variables)==\u001b[32m1\u001b[39m,\\\n\u001b[32m     21\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mthe dimesnion of input variables should be 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m   f1=chain[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m   f2=chain[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mAssertionError\u001b[39m: the dimesnion of input variables should be 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import List \n",
    "from typing import Callable \n",
    "\n",
    "function_type = Callable[[np.ndarray],np.ndarray]\n",
    "Chain = List[function_type]\n",
    "\n",
    "def deriv(func:function_type,input_param:np.ndarray,delta:float=0.001):\n",
    "  return func(input_param+delta)-func(input_param-delta)/(2*delta)\n",
    "\n",
    "def square(x:np.ndarray)->np.ndarray:\n",
    "  return x*x\n",
    "def sigmoid(x:np.ndarray)->np.ndarray:\n",
    "  return 1/(1-np.exp(-x))\n",
    "\n",
    "def derivativechain(chain:Chain,input_variables:np.ndarray)->np.ndarray:\n",
    "  assert len(chain)==2 , \\\n",
    "  \"the length of chain should be 2\"\n",
    "  assert np.ndim(input_variables)==1,\\\n",
    "  \"the dimesnion of input variables should be 1\"\n",
    "\n",
    "  f1=chain[0]\n",
    "  f2=chain[1]\n",
    "\n",
    "  f1_output = deriv(f1,input_variables)\n",
    "  f2_output = deriv(f2,f1_output)\n",
    "\n",
    "  return f2_output\n",
    "\n",
    "mychain = [square,sigmoid]\n",
    "\n",
    "derivativechain(mychain,np.ndarray([2,4,5,6,7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f67a2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6] --> [7.06508354e-02 9.00281627e-07 0.00000000e+00]\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "[2 4 6] -> [0.1849561  0.03469005 0.00492082]\n"
     ]
    }
   ],
   "source": [
    "from typing import List \n",
    "from typing import Callable\n",
    "import numpy as np  \n",
    "\n",
    "function_type = Callable[[np.ndarray],np.ndarray]\n",
    "Chain = List[function_type]\n",
    "\n",
    "def square(x:np.ndarray):\n",
    "  return np.pow(x,2)\n",
    "\n",
    "def sigmoid(x:np.ndarray):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def derivative(func:function_type,input_param:np.ndarray,delta:float=0.001)->np.ndarray:\n",
    "  return (func(input_param+delta)-func(input_param-delta))/(2*delta)\n",
    "\n",
    "def chain_differentiation(chain:Chain,input_variables:np.ndarray)->np.ndarray:\n",
    "  assert len(chain)==2 , \"chain length must be 2\"\n",
    "  assert np.ndim(input_variables)==1, \"dimension of input_variables must be 1\"\n",
    "\n",
    "  f1 = chain[0]\n",
    "  f2 = chain[1]\n",
    "\n",
    "  # f = f2(f1(x))\n",
    "  # d(f2(f1(x)))/d(x) = f2'f1(x)*f1'(x)\n",
    "  f1_x = f1(input_variables)\n",
    "  df1_dx = derivative(f1,input_variables)\n",
    "  df2_df1 = derivative(f2,f1_x)\n",
    "\n",
    "  chained_differentiation = df2_df1*df1_dx\n",
    "  return chained_differentiation\n",
    "\n",
    "chain_1 = [square,sigmoid]\n",
    "x = np.array([2,4,6])\n",
    "output_1 = chain_differentiation(chain_1,x)\n",
    "print(f\"{x} --> {output_1}\")\n",
    "print(\"*-\"*50)\n",
    "chain_2 =[sigmoid,square]\n",
    "output_2 = chain_differentiation(chain_2,x)\n",
    "print(f\"{x} -> {output_2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5252da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1: [7.06508354e-02 9.00281627e-07 0.00000000e+00]\n",
      "**************************************************\n",
      "output2: [7.06508354e-02 9.00281627e-07 0.00000000e+00]\n",
      "**************************************************\n",
      "output3: [0.1849561  0.03469005 0.00492082]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from typing import List , Callable \n",
    "import numpy as np \n",
    "function_type = Callable[[np.ndarray],np.ndarray]\n",
    "Chain= List[function_type]\n",
    "\n",
    "def square(x:np.ndarray)->np.ndarray:\n",
    "  return np.pow(x,2)\n",
    "def leaky_relu(x:np.ndarray,alpha:float=0.01)->np.ndarray:\n",
    "  return np.where(x>0,x,alpha*x)\n",
    "def sigmoid(x:np.ndarray)->np.ndarray:\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "def derivative(func:function_type,input_param:np.ndarray,delta:float=0.001)->np.ndarray:\n",
    "\n",
    "  return (func(input_param+delta)-func(input_param-delta))/(2*delta)\n",
    "\n",
    "def chain_derivative_size3(chain:Chain,x:np.ndarray)->np.ndarray:\n",
    "  assert len(chain)==3 , \"chain should be of size 3\"\n",
    "  assert np.ndim(x)==1, \"input variables should be of dimension 1\"\n",
    "  f1,f2,f3 = chain\n",
    "  # f = f3(f2(f1(x)))\n",
    "  # f' = f3'(f2(f1(x)))*f2'(f1(x))*f1'(x)\n",
    "  f1_x =f1(x)\n",
    "  f2_x = f2(f1_x)\n",
    "\n",
    "  d_f1_x = derivative(f1,x)\n",
    "  d_f2_f1_x = derivative(f2,f1_x)\n",
    "  d_f3_f2_f1_x = derivative(f3,f2_x) \n",
    "\n",
    "  chained_deriv = d_f3_f2_f1_x*d_f2_f1_x*d_f1_x\n",
    "  return chained_deriv\n",
    "\n",
    "x = np.array([2,4,6])\n",
    "chain_1 = [square,leaky_relu,sigmoid]\n",
    "output_1 = chain_derivative_size3(chain_1,x)\n",
    "print(f\"output1: {output_1}\")\n",
    "print(\"*\"*50)\n",
    "chain_2 = [leaky_relu,square,sigmoid]\n",
    "output_2 = chain_derivative_size3(chain_2,x)\n",
    "print(f\"output2: {output_2}\")\n",
    "print(\"*\"*50)\n",
    "chain_3 = [sigmoid,square,leaky_relu]\n",
    "output_3 = chain_derivative_size3(chain_3,x)\n",
    "print(f\"output3: {output_3}\")\n",
    "print(\"*\"*50)\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a414805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul_forward(X:np.ndarray,W:np.ndarray)->np.ndarray:\n",
    "  assert X.shape[1]==W.shape[0], \"the order didn't match for matrix multiplication\"\n",
    "  return np.dot(X,W)\n",
    "X = np.array([[2,3,4,5]])\n",
    "W = np.array([[3],[5,],[5],[4]])\n",
    "matmul_forward(X,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e6f64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_backward(X:np.ndarray,W:np.ndarray)->np.ndarray:\n",
    "  return np.transpose(W,(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bab81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_function_backward_sum(\n",
    "    X: np.ndarray,\n",
    "    W: np.ndarray,\n",
    "    sigma: Callable[[np.ndarray], np.ndarray]\n",
    ") -> np.ndarray:\n",
    "\n",
    "    assert X.shape[1] == W.shape[0]\n",
    "\n",
    "    # forward\n",
    "    N = X @ W\n",
    "    S = sigma(N)\n",
    "    L = np.sum(S)\n",
    "\n",
    "    # backward\n",
    "    dLdS = np.ones_like(S)          # d(sum)/dS\n",
    "    dSdN = deriv(sigma, N)          # sigma'(N)\n",
    "    dLdN = dLdS * dSdN              # chain rule\n",
    "\n",
    "    dNdX = W.T                      # derivative of XW wrt X\n",
    "    dLdX = dLdN @ dNdX              # final gradient @ means dot product\n",
    "\n",
    "    return dLdX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68001935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear Regression from Scratch (No ML libraries)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "# =====================================================\n",
    "# 1️⃣ Forward Pass\n",
    "# =====================================================\n",
    "def forward_pass(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    weights: Dict[str, np.ndarray]\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs forward propagation.\n",
    "\n",
    "    X : (n_samples, n_features)\n",
    "    Y : (n_samples, 1)\n",
    "    w : (n_features, 1)\n",
    "    b : (1,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Linear transformation\n",
    "    # N = XW\n",
    "    N = X @ weights['w']\n",
    "\n",
    "    # Add bias\n",
    "    # P = XW + b\n",
    "    P = N + weights['b']\n",
    "\n",
    "    # Mean Squared Error Loss\n",
    "    loss = np.mean((Y - P) ** 2)\n",
    "\n",
    "    # Store values needed for backpropagation\n",
    "    cache = {\n",
    "        'X': X,\n",
    "        'Y': Y,\n",
    "        'P': P\n",
    "    }\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2️⃣ Backward Pass (Gradient Calculation)\n",
    "# =====================================================\n",
    "def backward_pass(\n",
    "    cache: Dict[str, np.ndarray],\n",
    "    weights: Dict[str, np.ndarray]\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes gradients of loss w.r.t weights and bias\n",
    "    using chain rule.\n",
    "    \"\"\"\n",
    "\n",
    "    X = cache['X']\n",
    "    Y = cache['Y']\n",
    "    P = cache['P']\n",
    "    n = X.shape[0]  # number of samples\n",
    "\n",
    "    # dL/dP = derivative of MSE loss\n",
    "    dL_dP = (-2 / n) * (Y - P)          # shape: (n, 1)\n",
    "\n",
    "    # dL/dW = Xᵀ · dL/dP\n",
    "    dL_dW = X.T @ dL_dP                 # shape: (features, 1)\n",
    "\n",
    "    # dL/dB = sum(dL/dP)\n",
    "    dL_dB = np.sum(dL_dP, axis=0)       # shape: (1,)\n",
    "\n",
    "    gradients = {\n",
    "        'w': dL_dW,\n",
    "        'b': dL_dB\n",
    "    }\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3️⃣ Training Loop (Gradient Descent)\n",
    "# =====================================================\n",
    "def train(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    epochs: int = 1000,\n",
    "    lr: float = 0.000001\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains linear regression using gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Initialize weights with small random values\n",
    "    weights = {\n",
    "        'w': np.random.randn(n_features, 1) * 0.01,\n",
    "        'b': np.zeros(1)\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Forward pass\n",
    "        loss, cache = forward_pass(X, Y, weights)\n",
    "\n",
    "        # Backward pass\n",
    "        gradients = backward_pass(cache, weights)\n",
    "\n",
    "        # Update parameters\n",
    "        weights['w'] -= lr * gradients['w']\n",
    "        weights['b'] -= lr * gradients['b']\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {loss:.2f}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4️⃣ Prediction\n",
    "# =====================================================\n",
    "def predict(X: np.ndarray, weights: Dict[str, np.ndarray]):\n",
    "    \"\"\"\n",
    "    Makes predictions using trained weights\n",
    "    \"\"\"\n",
    "    return X @ weights['w'] + weights['b']\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5️⃣ Realistic Dataset (House Prices)\n",
    "# =====================================================\n",
    "\"\"\"\n",
    "Features:\n",
    "- House size (sqft)\n",
    "- Number of bedrooms\n",
    "\n",
    "Target:\n",
    "- House price (in dollars)\n",
    "\"\"\"\n",
    "\n",
    "X = np.array([\n",
    "    [800,  1],\n",
    "    [1000, 2],\n",
    "    [1200, 2],\n",
    "    [1500, 3],\n",
    "    [1800, 3],\n",
    "    [2000, 4],\n",
    "    [2300, 4],\n",
    "    [2600, 5]\n",
    "], dtype=float)\n",
    "\n",
    "Y = np.array([\n",
    "    [120000],\n",
    "    [150000],\n",
    "    [180000],\n",
    "    [220000],\n",
    "    [260000],\n",
    "    [300000],\n",
    "    [340000],\n",
    "    [380000]\n",
    "], dtype=float)\n",
    "\n",
    "# =====================================================\n",
    "# 6️⃣ Train the Model\n",
    "# =====================================================\n",
    "trained_weights = train(X, Y, epochs=2000, lr=1e-7)\n",
    "\n",
    "print(\"\\nTrained Weights:\")\n",
    "print(\"W:\", trained_weights['w'])\n",
    "print(\"b:\", trained_weights['b'])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7️⃣ Test Prediction\n",
    "# =====================================================\n",
    "# New house: 1700 sqft, 3 bedrooms\n",
    "X_test = np.array([[1700, 3]], dtype=float)\n",
    "\n",
    "prediction = predict(X_test, trained_weights)\n",
    "print(f\"\\nPredicted price for 1700 sqft, 3BHK house: ${prediction[0,0]:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dadc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigma(x:np.ndarray)->np.ndarray:\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def forward_loss(X:np.ndarray,y:np.ndarray,weights:Dict[str,np.ndarray])->Tuple[Dict[str,np.ndarray],float]:\n",
    "  '''Compute the forward pass and the loss for the step by step neural network model'''\n",
    "  \n",
    "  M1=X@weights['W1'] # n,f1 f1,1 -> n,1\n",
    "  N1=M1+weights['B1'] # n,1\n",
    "  O1=sigma(N1) # n,1\n",
    "  M2=O1@weights['W2'] # n,1 1,1 -> n,1\n",
    "  P=M2+weights['B2'] # n,1\n",
    "  L=np.mean(np.power(y-P),2) # n,1\n",
    "\n",
    "  forward_info={}\n",
    "  forward_info['X']=X\n",
    "  forward_info['y']=y\n",
    "  forward_info[\"M1\"]=M1\n",
    "  forward_info[\"N1\"]=N1\n",
    "  forward_info[\"O1\"]=O1\n",
    "  forward_info[\"M2\"]=M2\n",
    "  forward_info[\"P\"]=P\n",
    "\n",
    "  return (forward_info,L)\n",
    "\n",
    "def gradient_calculation(forward_info:Dict[str,np.ndarray],weights:Dict[str,np.ndarray])->Dict[str,float]:\n",
    "  # dLdW1 = -2(y-p)*1_m2*w2T*sigma(n1).(1-sigma(n1))*1_m1*xT\n",
    "  # dLdW2 = -2(y-p)*1_m2*o1T\n",
    "  # dLdB1 = -2(y-p)*1_m2*w2T*sigma(n1).(1-sigma(n1))*1_b1\n",
    "  # dLdB2 = -2(y-p)*1_b2\n",
    "  x=forward_info['X']\n",
    "  y=forward_info['y']\n",
    "  p=forward_info['P']\n",
    "  m2=forward_info['M2']\n",
    "  n1=forward_info['N1']\n",
    "  m1=forward_info[\"M1\"]\n",
    "  o1=forward_info['O1']\n",
    "  w1=weights['W1']\n",
    "  w2=weights['W2']\n",
    "  b1=weights['B1']\n",
    "  b2=weights['B2']\n",
    "\n",
    "  dLdP=-2*(y-p)/forward_info[\"y\"].shape[0]\n",
    "  dPdM2=np.ones_like(m2)\n",
    "  dM2dO1=w2.T\n",
    "  dO1dN1=sigma(n1)*(1-sigma(n1))\n",
    "  dN1dM1=np.ones_like(m1)\n",
    "  dM1dW1=x.T\n",
    "  dM2dW2=o1.T\n",
    "  dN1dB1=np.ones_like(b1)\n",
    "  dPdB2 = np.ones_like(b2)\n",
    "\n",
    "  dLdW1 = dLdP*dPdM2*dM2dO1*dO1dN1*dN1dM1*dM1dW1\n",
    "  dLdW2 = dLdP*dPdM2*dM2dW2\n",
    "  dLdB1 = dLdP*dPdM2*dM2dO1*dO1dN1*dN1dB1\n",
    "  dLdB2 = dLdP*dPdB2\n",
    "\n",
    "  gradient_dict={}\n",
    "  gradient_dict['dLdW1']=dLdW1\n",
    "  gradient_dict['dLdW2']=dLdW2\n",
    "  gradient_dict['dLdB1']=dLdB1\n",
    "  gradient_dict['dLdB2']=dLdB2\n",
    "\n",
    "  return gradient_dict\n",
    "\n",
    "def train_model(x_batch:np.ndarray,y_batch:np.ndarray,epochs=int,learning_rate:float=0.001)->Dict[str,np.ndarray]:\n",
    "  weights = Dict[str,np.ndarray]\n",
    "  weights['W1'] = np.random.randn(x_batch.shape[1],1)\n",
    "  weights['W2'] = np.random.randn(1,1)\n",
    "  weights['B1'] = np.zeros(1)\n",
    "  weights['B2'] = np.zeros(1)\n",
    "\n",
    "  for i in range(epochs):\n",
    "    forward_info,loss_cal = forward_loss(X=x_batch,y=y_batch,weights=weights)\n",
    "    dLdW1,dLdW2,dLdB1,dLdB2 = gradient_calculation(forward_info=forward_info,weights=weights)\n",
    "    weights['W1'] -= learning_rate*dLdW1\n",
    "    weights['W2'] -= learning_rate*dLdW2\n",
    "    weights['B1'] -= learning_rate*dLdB1\n",
    "    weights['B2'] -= learning_rate*dLdB2\n",
    "\n",
    "    if (i%10)==0:\n",
    "      print(f\"epoch {i+1} loss: {loss_cal}\")\n",
    "\n",
    "    return weights\n",
    "  \n",
    "  def predict(x_test:np.ndarray,weights:Dict[str,float]):\n",
    "    return sigma(x_test@weights['W1']+weights['B1'])@weights['W2']+weights['B2']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f05ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c78af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray \n",
    "from typing import List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ada8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array:ndarray,array_grad:ndarray):\n",
    "  assert array.shape == array_grad.shape ,(\n",
    "    f\"two ndarrays shape should be same\"\n",
    "    f\"first ndarray shape is {array.shape}\"\n",
    "    f\"second ndarray shape is {array_grad.shape}\"\n",
    "  )\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea00631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Operation \n",
    "class Operation(object):\n",
    "  '''\n",
    "  Base class fore every operation of the neural network\n",
    "  '''\n",
    "  def _output(self)->ndarray:\n",
    "    '''must be implemented in every \"Operation\" class '''\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "  def _input_grad(self,output_grad:ndarray)->ndarray:\n",
    "    '''must be implemented in every \"Operation\" class'''\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "  def forward(self,input_:ndarray):\n",
    "    '''calculates forward pass output'''\n",
    "    self._input = input_\n",
    "    self.output = self._output(self._input)\n",
    "    return self.output \n",
    "  \n",
    "  def backward(self,output_grad:ndarray)->ndarray:\n",
    "    '''calculates the input gradient for given output gradient'''\n",
    "    assert_same_shape(self.output,output_grad)\n",
    "    self.input_grad = self._input_grad(output_grad)\n",
    "    assert_same_shape(self._input,self.input_grad)\n",
    "    return self.input_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d4a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "  '''An Operation with parameters'''\n",
    "\n",
    "  def __init__(self,param:ndarray)->ndarray:\n",
    "    '''The param Operation Method'''\n",
    "    super().__init__()\n",
    "    self.param = param\n",
    "\n",
    "  def _param_grad(self,output_grad:ndarray):\n",
    "    '''define the operation for _param_grad caculation'''\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def backward(self,output_grad:ndarray)->ndarray:\n",
    "    '''Calls self._input_grad and self._param_grad. checks appropriate shape'''\n",
    "    assert_same_shape(self.output,output_grad)\n",
    "\n",
    "    self.input_grad = self._input_grad(output_grad)\n",
    "    self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "    assert_same_shape(self._input,self.input_grad)\n",
    "    assert_same_shape(self.param,self.param_grad)\n",
    "\n",
    "    return self.input_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ca951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Operations \n",
    "\n",
    "class WeightMultiply(ParamOperation):\n",
    "  '''Weight mulitplication operation for a neural network'''\n",
    "\n",
    "  def __init__(self,W:ndarray):\n",
    "    '''initialize operation with self.param = W'''\n",
    "    super()._init__(W)\n",
    "\n",
    "\n",
    "  def _output(self)->ndarray:\n",
    "    '''compute Output'''\n",
    "    return np.dot(self._input,self.param)\n",
    "  \n",
    "  def _input_grad(self,output_grad:ndarray)->ndarray:\n",
    "    '''compute input gradient'''\n",
    "    return np.dot(output_grad,np.transpose(self.param,(1,0))) # need to understand\n",
    "  \n",
    "  def _param_grad(self,output_grad:ndarray)->ndarray:\n",
    "    '''compute param gradient'''\n",
    "    return np.dot(np.transpose(self._input,(1,0)),output_grad) # need to understand \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dfdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    " '''Compute bias addition'''\n",
    " def __init__(self,B:ndarray):\n",
    "  '''initialize Operation with self.param=B'''\n",
    "  assert B.shape[0]==1 \n",
    "  super().__init__(B)\n",
    "\n",
    " def _output(self)->ndarray:\n",
    "  '''compute output'''\n",
    "  return self.input_ + self.param \n",
    " \n",
    " def _input_grad(self,output_grad:ndarray)->ndarray:\n",
    "  '''compute input gradient'''\n",
    "  return np.ones_like(self._input)*output_grad\n",
    " \n",
    " def _param_grad(self,output_grad:ndarray)->ndarray:\n",
    "  '''compute parameter gradient'''\n",
    "  param_grad = np.ones_like(self.param)*output_grad \n",
    "  return np.sum(param_grad,axis=0).reshape(1,param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1698f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "  '''Sigmoid activation function'''\n",
    "  def __init__(self)->None:\n",
    "    '''pass'''\n",
    "    super().__init__()\n",
    "\n",
    "  def _output(self)->ndarray:\n",
    "    '''compute output'''\n",
    "    return 1.0/(1.0 + np.exp(-1.0*self.input_))\n",
    "  \n",
    "  def _input_grad(self,output_grad:ndarray)->ndarray:\n",
    "    '''compute input gradient'''\n",
    "    sigmoid_backward = self.output * (1.0-self.output)\n",
    "    input_grad = sigmoid_backward*output_grad \n",
    "    return input_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6d1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "  '''Identify activation function'''\n",
    "  def __init__(self)->None:\n",
    "    '''pass'''\n",
    "    super().__init__()\n",
    "\n",
    "  def _output(self)->ndarray:\n",
    "    '''pass through'''\n",
    "    return self.input_ \n",
    "  \n",
    "  def _input_grad(self,output_grad:ndarray)->ndarray:\n",
    "    '''Pass through'''\n",
    "    return output_grad \n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7207c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layer and Dense \n",
    "\n",
    "class Layer(object):\n",
    "  '''a layer of neurons in a neural network'''\n",
    "  def __init__(self,neurons:int):\n",
    "    '''The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer'''\n",
    "    self.neurons = neurons \n",
    "    self.first = True\n",
    "    self.params:List[ndarray]=[]\n",
    "    self.params_grads:List[ndarray]=[]\n",
    "    self.operations:List[Operation]=[]\n",
    "\n",
    "  def _setupt_layer(self,num_in:int)->None:\n",
    "    ''' The _setup_layer function must be implemented for each layer'''\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "  def forward(self,input_:ndarray)->ndarray:\n",
    "    '''Passes input forward through a series of operations'''\n",
    "    if self.first:\n",
    "      self._setup_layer(input_)\n",
    "      self.first=False \n",
    "    self._input = input_\n",
    "    for operation in self.operations:\n",
    "      input_ = operation.forward(input_)\n",
    "\n",
    "    self.output = input_\n",
    "    return self.output \n",
    "  \n",
    "\n",
    "  def backward(self,output_grad:ndarray)->ndarray:\n",
    "    '''passes output_grad backward through a series of operations '''\n",
    "    assert_same_shape(self.output,output_grad)\n",
    "    for operation in reversed(self.operations):\n",
    "      output_grad = operation.backward(output_grad)\n",
    "\n",
    "    input_grad = output_grad \n",
    "    self.params_grads()\n",
    "    return input_grad \n",
    "  \n",
    "  def _param_grads(self)->ndarray:\n",
    "    '''Extracts the _param_grads from a layers' operation'''\n",
    "    self.params_grads = []\n",
    "    for operation in self.operations:\n",
    "      if issubclass(operation.__class__,ParamOperation):\n",
    "        self.params_grads.append(operation.param_grad)\n",
    "\n",
    "  def _params(self)->ndarray:\n",
    "    '''Extracts the _param_grads from a layers' operations'''\n",
    "    self.params_grads = []\n",
    "    for operation in self.operations:\n",
    "      if issubclass(operation.__class__,ParamOperation):\n",
    "        self.params.append(operation.param)\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00c2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "  '''A fully connected layer which inherits from \"layer\"'''\n",
    "  def __init__(self,neurons:int,activation:Operation=Sigmoid()):\n",
    "    '''requires an activation function upon initialization'''\n",
    "    super().__init__(neurons)\n",
    "    self.activation = activation \n",
    "\n",
    "  def _setup_layer(self,input_:ndarray)->None:\n",
    "    '''Defines the operations of a fully connected layer'''\n",
    "    if self.seed:\n",
    "      np.random.seed(self.seed)\n",
    "\n",
    "    self.params = []\n",
    "\n",
    "    self.params.append(np.random(input_.shape[1],self.neurons))\n",
    "    self.params.append(np.random.randn(1,self.neurons))\n",
    "    self.operations = [WeightMultiply(self.params[0]),BiasAdd(self.params[1]),self.activation]\n",
    "\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6f73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "  ''' The \"loss\" of a neural network'''\n",
    "\n",
    "  def __init__(self):\n",
    "    '''pass'''\n",
    "    pass \n",
    "  def forward(self,prediction:ndarray,target:ndarray)->float:\n",
    "    '''computes the actual loss value'''\n",
    "    assert_same_shape(prediction,target)\n",
    "    self.prediction = prediction \n",
    "    self.target = target \n",
    "\n",
    "    loss_value = self._output()\n",
    "    return loss_value \n",
    "  \n",
    "  def backward(self)->ndarray:\n",
    "    '''computes gradient of the loss value with respect to the input to the loss function '''\n",
    "    self.input_grad = self._input_grad()\n",
    "    assert_same_shape(self.prediction,self.input_grad)\n",
    "    return self.input_grad \n",
    "  \n",
    "  def _output(self)->float:\n",
    "    '''Every sublass of \"Loss\" must implement the _output function'''\n",
    "    raise NotImplementedError()\n",
    "  \n",
    "  def _input_grad(self)->ndarray:\n",
    "    '''Every subclass of \"Loss\" must implement the _input_grad function'''\n",
    "    raise NotImplementedError()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d29a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "  def __init__(self)->None:\n",
    "    '''pass'''\n",
    "    super().__init__()\n",
    "\n",
    "  def _output(self)->float:\n",
    "    '''Computes the per-observation squared error loss'''\n",
    "    loss = (np.sum(np.power(self.prediction-self.target,2)))/self.prediction.shape[0]\n",
    "    return loss \n",
    "  \n",
    "  def _input_grad(self)->ndarray:\n",
    "    '''Computes the loss gradient with respect to the input for MSE loss'''\n",
    "    return 2.0*(self.prediction-self.target)/self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e8c6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## neuralnetwork \n",
    "\n",
    "class NeuralNetwork(object):\n",
    "  '''The class for a neural network'''\n",
    "  def __init__(self,layers:List[Layer],loss:Loss,seed:int=1)->None:\n",
    "    '''Neural network needs layers and loss'''\n",
    "    self.layers = layers \n",
    "    self.loss = loss \n",
    "    self.seed = seed \n",
    "    if seed:\n",
    "      for layer in self.layers:\n",
    "        setattr(layer,\"seed\",self.seed)\n",
    "\n",
    "  def forward(self,x_batch:ndarray)->ndarray:\n",
    "    '''passes data forward through a series of layers'''\n",
    "    x_out = x_batch \n",
    "    for layer in self.layers:\n",
    "      x_out = layer.forward(x_out)\n",
    "\n",
    "      return x_out \n",
    "    \n",
    "  def backward(self,loss_grad:ndarray)->None:\n",
    "    '''passes data backward through a series of layers'''\n",
    "    grad = loss_grad \n",
    "    for layer in reversed(self.layers):\n",
    "      grad = layer.backward(grad)\n",
    "\n",
    "    return None \n",
    "  \n",
    "  def train_batch(self,x_batch:ndarray,y_batch:ndarray)->float:\n",
    "    '''passes data forward through the layers . computes the loss passes data backward through the layers '''\n",
    "\n",
    "    predictions = self.forward(x_batch)\n",
    "    loss = self.loss.forward(predictions,y_batch)\n",
    "    self.backward(self.loss.backward())\n",
    "    return loss \n",
    "  \n",
    "  def params(self):\n",
    "    '''get teh parameters for the network'''\n",
    "    for layer in self.layers:\n",
    "      yield from layer.params \n",
    "\n",
    "  def params_grad(self):\n",
    "    '''gets the gradient of the loss with respect to the parameters for the network '''\n",
    "    for layer in self.layers:\n",
    "      yield from layer.param_grads \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e81fef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and sgd \n",
    "class Optimizer(object):\n",
    "  '''Base class for a neural network optimizer'''\n",
    "  def __init__(self,lr:float=0.01):\n",
    "    '''Every optimizer must have an initial learning rate'''\n",
    "    self.lr = lr \n",
    "\n",
    "  def step(self)->None : \n",
    "    '''every optimizer must implement the step function '''\n",
    "    pass \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f99b0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "  '''stochastic gradient descent optimizer'''\n",
    "  def __init__(self,lr:float=0.01)->None:\n",
    "    '''pass'''\n",
    "    super().__init__(lr)\n",
    "\n",
    "  def step(self):\n",
    "    '''for each parameter , adjust in the appropriate direction,  with magnitude of the adjustment based on the learning rate.'''\n",
    "    for (param,param_grad) in zip(self.net.params(),self.net.param_grads()):\n",
    "      param-=self.lr*param_grad \n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1cb197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer \n",
    "from copy import deepcopy \n",
    "from typing import Tuple \n",
    "\n",
    "class Trainer(object):\n",
    "  '''trains a neural network'''\n",
    "  def __init__(self,net:NeuralNetwork,optim:Optimizer)->None:\n",
    "    '''requires a neural network and an optimizer in order for training to occur . assign the neural network as an instance variable to the optimizer'''\n",
    "    self.net = net \n",
    "    self.optim = optim \n",
    "    self.best_loss = 1e9\n",
    "    setattr(self.optim,'net',self.net)\n",
    "  def generate_batches(self,X:ndarray,y:ndarray,size:int=32)->Tuple[ndarray]:\n",
    "    '''generates batches for training'''\n",
    "    assert X.shape[0] == y.shape[0],\"features and target must have the same number of rows, instead features has {0} and target has {1}\".format(X.shape[0],y.shape[0])\n",
    "    N = X.shape[0]\n",
    "\n",
    "    for ii in range(0,N,size):\n",
    "      X_batch,y_batch = X[ii:ii+size],y[ii:ii+size]\n",
    "      yield X_batch,y_batch \n",
    "\n",
    "  def fit(self,X_train:ndarray,y_train:ndarray,X_test:ndarray,y_test:ndarray,epochs:int=100,eval_every:int=10,batch_size:int=1,seed:int=1,restart:bool=True)->None :\n",
    "\n",
    "    '''fits the neural network on the training data for certain number of epochs . every \"eval_every\" epochs, it evaluated the neural network on the testing data.'''\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if restart:\n",
    "      for layer in self.net.layers:\n",
    "        layer.first = True \n",
    "\n",
    "      self.best_loss = 1e9 \n",
    "\n",
    "    for e in range(epochs):\n",
    "      if(e+1)% eval_every == 0:\n",
    "        last_model = deepcopy(self.net)\n",
    "\n",
    "      X_train,y_train = permute_data(X_train,y_train)\n",
    "      batch_generator = self.generate_batches(X_train,y_train,batch_size)\n",
    "\n",
    "      for ii,(X_batch,y_batch) in enumerate(batch_generator):\n",
    "        self.net.train_batch(X_batch,y_batch)\n",
    "        self.optim.step()\n",
    "\n",
    "      if (e+1) % eval_every == 0:\n",
    "        test_preds = self.net.forward(X_test)\n",
    "        loss = self.net.loss.forward(test_preds,y_test)\n",
    "\n",
    "        if loss<self.best_loss:\n",
    "          print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "          self.best_loss = loss \n",
    "        else:\n",
    "          print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f},using the model from epoch {e+1-eval_every}\"\"\")\n",
    "          self.net = last_model \n",
    "          setattr(self.optim,'net',self.net)\n",
    "          break \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d574553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition using '__add__' :  [7 7]\n",
      "addition using '+':  [7 7]\n"
     ]
    }
   ],
   "source": [
    "# coding up gradient accumulation \n",
    "import numpy as np \n",
    "\n",
    "a = np.array([3,3])\n",
    "print(\"addition using '__add__' : \",a.__add__(4))\n",
    "print(\"addition using '+': \",a+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union , List \n",
    "Numberable = Union[int,float,\"NumberWithGrad\"]\n",
    "class NumberWithGrad:\n",
    "  def __init__(\n",
    "      self,\n",
    "      num:Union[int,float],\n",
    "      depends_on:List[\"NumberWithGrad\"]=None,\n",
    "      creation_op:str=\"\"\n",
    "  ):\n",
    "    self.num = num \n",
    "    self.grad=0.0 \n",
    "    self.depends_on = depends_on or []\n",
    "    self.creation_op = creation_op \n",
    "\n",
    "\n",
    "    \n",
    "  def __add__(self,other:Numberable)->\"NumberWithGrad\":\n",
    "    other = ensure_number(other)\n",
    "    return NumberWithGrad(\n",
    "      self.num + other.num,\n",
    "      depends_on=[self,other],\n",
    "      creation_op=\"add\"\n",
    "    )\n",
    "  \n",
    "  def __mul__(self,other:Numberable)->\"NumberWithGrad\":\n",
    "    other = ensure_number(other)\n",
    "    return NumberWithGrad(\n",
    "        self.num * other,\n",
    "        depends_on=[self,other],\n",
    "        creation_op=\"mul\"\n",
    "      )\n",
    "    \n",
    "  \n",
    "def ensure_number(num:Numberable)->\"NumberWithGrad\":\n",
    "  if isinstance(num,NumberWithGrad):\n",
    "    return num \n",
    "  else:\n",
    "    return NumberWithGrad(num)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self,backward_grad:Numberable=None)->None:\n",
    "  if backward_grad is None:\n",
    "    self.grad=1 \n",
    "\n",
    "  else:\n",
    "    if self.grad is None:\n",
    "      self.grad = backward_grad \n",
    "    else:\n",
    "      self.grad += backward_grad \n",
    "\n",
    "  if self.creation_op == \"add\":\n",
    "    self.depends_on[0].backward(self,grad)\n",
    "    self.depends_on[1].backward(self,grad)\n",
    "\n",
    "  if self.creation_op==\"mul\":\n",
    "    new = self.depends_on[1]*self.grad \n",
    "    self.depends_on[0].backward(new.num)\n",
    "\n",
    "    new = self.depends_on[0]*self.grad \n",
    "    self.depends_on[1].backward(new.num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
