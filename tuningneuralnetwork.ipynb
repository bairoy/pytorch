{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "641671d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.model_selection import train_test_split \n",
    "import optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fad0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100d7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=1000,n_features=10,n_classes=2,random_state=1)\n",
    "X_train,X_val,y_train,y_val = train_test_split(\n",
    "  X,y,test_size=0.2,random_state=1\n",
    ")\n",
    "X_train = torch.tensor(X_train,dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val,dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32).view(-1,1)\n",
    "y_val = torch.tensor(y_val,dtype=torch.float32).view(-1,1)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "  TensorDataset(X_train,y_train),\n",
    "  batch_size=64,\n",
    "  shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "  TensorDataset(X_val,y_val),\n",
    "  batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a718559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "  def __init__(self,dropout):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(10,32),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dropout),\n",
    "      nn.Linear(32,1)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b15563c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model,loader,optimizer,criterion,device):\n",
    "  model.train()\n",
    "  total_loss = 0.0\n",
    "  for x,y in loader:\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "  return total_loss/len(loader)\n",
    "\n",
    "def validate(model,loader,criterion,device):\n",
    "  model.eval()\n",
    "  total_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for x,y in loader:\n",
    "      x,y = x.to(device),y.to(device)\n",
    "      logits = model(x)\n",
    "      loss = criterion(logits,y)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "  return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04d6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  lr = trial.suggest_float(\"lr\",1e-4,1e-2,log=True)\n",
    "  dropout = trial.suggest_float(\"dropout\",0.1,0.5)\n",
    "  weight_decay = trial.suggest_float(\"weight_decay\",1e-6,1e-3,log=True)\n",
    "\n",
    "  model = BinaryClassifier(dropout=dropout).to(device)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    "    weight_decay = weight_decay\n",
    "  )\n",
    "\n",
    "  max_epochs = 20\n",
    "  for epoch in range(max_epochs):\n",
    "    train_loss = train_one_epoch(\n",
    "      model,train_loader,optimizer,criterion,device\n",
    "    )\n",
    "    val_loss = validate(\n",
    "      model,val_loader,criterion,device\n",
    "    )\n",
    "\n",
    "    trial.report(val_loss,step=epoch)\n",
    "\n",
    "    if trial.should_prune():\n",
    "      raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dfafe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-07 18:38:44,802] A new study created in memory with name: pytorch_finetuning\n",
      "[I 2026-01-07 18:38:44,922] Trial 0 finished with value: 0.680860847234726 and parameters: {'lr': 0.00010683707321166706, 'dropout': 0.49569669331132377, 'weight_decay': 1.481930668407028e-06}. Best is trial 0 with value: 0.680860847234726.\n",
      "[I 2026-01-07 18:38:44,930] Trial 1 finished with value: 0.6451137363910675 and parameters: {'lr': 0.0017821063190525883, 'dropout': 0.14128435173090123, 'weight_decay': 1.8171318979874065e-05}. Best is trial 1 with value: 0.6451137363910675.\n",
      "[I 2026-01-07 18:38:44,940] Trial 2 finished with value: 0.7412804961204529 and parameters: {'lr': 0.0001080574953703763, 'dropout': 0.16571340520737118, 'weight_decay': 5.366284459125061e-06}. Best is trial 1 with value: 0.6451137363910675.\n",
      "[I 2026-01-07 18:38:44,950] Trial 3 finished with value: 0.6523437947034836 and parameters: {'lr': 0.003587484974746162, 'dropout': 0.28959115646750205, 'weight_decay': 1.3796600457162046e-05}. Best is trial 1 with value: 0.6451137363910675.\n",
      "[I 2026-01-07 18:38:44,957] Trial 4 finished with value: 0.6144300103187561 and parameters: {'lr': 0.006537590208163734, 'dropout': 0.3448286108802969, 'weight_decay': 0.0002073290347819707}. Best is trial 4 with value: 0.6144300103187561.\n",
      "[I 2026-01-07 18:38:44,969] Trial 5 pruned. \n",
      "[I 2026-01-07 18:38:44,977] Trial 6 finished with value: 0.5019574090838432 and parameters: {'lr': 0.007143692349217159, 'dropout': 0.4218018355200417, 'weight_decay': 5.940749318710528e-06}. Best is trial 6 with value: 0.5019574090838432.\n",
      "[I 2026-01-07 18:38:44,991] Trial 7 finished with value: 0.583691194653511 and parameters: {'lr': 0.007706688598283129, 'dropout': 0.44736958769298674, 'weight_decay': 0.0002694284710906628}. Best is trial 6 with value: 0.5019574090838432.\n",
      "[I 2026-01-07 18:38:45,003] Trial 8 finished with value: 0.6248964965343475 and parameters: {'lr': 0.003638591246947601, 'dropout': 0.2929112106007062, 'weight_decay': 0.00033088053858051637}. Best is trial 6 with value: 0.5019574090838432.\n",
      "[I 2026-01-07 18:38:45,016] Trial 9 pruned. \n",
      "[I 2026-01-07 18:38:45,029] Trial 10 pruned. \n",
      "[I 2026-01-07 18:38:45,038] Trial 11 finished with value: 0.526643693447113 and parameters: {'lr': 0.009616645439566309, 'dropout': 0.39712582032974764, 'weight_decay': 0.0009185080991826233}. Best is trial 6 with value: 0.5019574090838432.\n",
      "[I 2026-01-07 18:38:45,048] Trial 12 finished with value: 0.495209664106369 and parameters: {'lr': 0.00925388773117409, 'dropout': 0.4135626239904558, 'weight_decay': 0.0009553947057651453}. Best is trial 12 with value: 0.495209664106369.\n",
      "[I 2026-01-07 18:38:45,058] Trial 13 pruned. \n",
      "[I 2026-01-07 18:38:45,069] Trial 14 pruned. \n",
      "[I 2026-01-07 18:38:45,080] Trial 15 finished with value: 0.5358467549085617 and parameters: {'lr': 0.0050184461242643846, 'dropout': 0.2512145445500674, 'weight_decay': 0.00010613978134849285}. Best is trial 12 with value: 0.495209664106369.\n",
      "[I 2026-01-07 18:38:45,089] Trial 16 finished with value: 0.5969637781381607 and parameters: {'lr': 0.0022128576646325455, 'dropout': 0.4290483886153653, 'weight_decay': 4.75379289142163e-06}. Best is trial 12 with value: 0.495209664106369.\n",
      "[I 2026-01-07 18:38:45,102] Trial 17 pruned. \n",
      "[I 2026-01-07 18:38:45,111] Trial 18 finished with value: 0.5341274738311768 and parameters: {'lr': 0.009291297207479277, 'dropout': 0.23963257924847942, 'weight_decay': 1.0677452522818657e-06}. Best is trial 12 with value: 0.495209664106369.\n",
      "[I 2026-01-07 18:38:45,121] Trial 19 pruned. \n",
      "[I 2026-01-07 18:38:45,134] Trial 20 finished with value: 0.5857174396514893 and parameters: {'lr': 0.005081171031512714, 'dropout': 0.3690597647009303, 'weight_decay': 2.8418312747131706e-06}. Best is trial 12 with value: 0.495209664106369.\n",
      "[I 2026-01-07 18:38:45,143] Trial 21 finished with value: 0.4898885264992714 and parameters: {'lr': 0.009543829378069682, 'dropout': 0.4073293949723712, 'weight_decay': 0.0009690611547489498}. Best is trial 21 with value: 0.4898885264992714.\n",
      "[I 2026-01-07 18:38:45,155] Trial 22 finished with value: 0.5409969985485077 and parameters: {'lr': 0.0054477773826228405, 'dropout': 0.4656171505910973, 'weight_decay': 0.0007279811598339407}. Best is trial 21 with value: 0.4898885264992714.\n",
      "[I 2026-01-07 18:38:45,167] Trial 23 finished with value: 0.5108193457126617 and parameters: {'lr': 0.009164914024366148, 'dropout': 0.426438323711769, 'weight_decay': 0.00038958691862229394}. Best is trial 21 with value: 0.4898885264992714.\n",
      "[I 2026-01-07 18:38:45,175] Trial 24 pruned. \n",
      "[I 2026-01-07 18:38:45,185] Trial 25 finished with value: 0.5367593467235565 and parameters: {'lr': 0.0053834619166758645, 'dropout': 0.37546384312905867, 'weight_decay': 0.0004913253357560007}. Best is trial 21 with value: 0.4898885264992714.\n",
      "[I 2026-01-07 18:38:45,195] Trial 26 pruned. \n",
      "[I 2026-01-07 18:38:45,378] Trial 27 pruned. \n",
      "[I 2026-01-07 18:38:45,388] Trial 28 pruned. \n",
      "[I 2026-01-07 18:38:45,399] Trial 29 pruned. \n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "  direction=\"minimize\",\n",
    "  study_name=\"pytorch_finetuning\"\n",
    ")\n",
    "study.optimize(objective,n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec54f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-07 18:43:44,618] A new study created in memory with name: no-name-55dbdcca-b4cd-443e-b44c-4142808c36c0\n",
      "[I 2026-01-07 18:43:45,391] Trial 0 finished with value: 0.15571239348500968 and parameters: {'n_layers': 4, 'hidden_size': 64, 'activation': 'leaky_relu', 'dropout': 0.23636207760646727, 'lr': 0.0012840930119041073, 'weight_decay': 0.00012356647671634002, 'optimizer': 'AdamW', 'batch_size': 32}. Best is trial 0 with value: 0.15571239348500968.\n",
      "[I 2026-01-07 18:43:45,611] Trial 1 finished with value: 0.37770040035247804 and parameters: {'n_layers': 1, 'hidden_size': 32, 'activation': 'elu', 'dropout': 0.48584896856264354, 'lr': 0.0020122929868617704, 'weight_decay': 6.704132518734996e-06, 'optimizer': 'Adam', 'batch_size': 64}. Best is trial 0 with value: 0.15571239348500968.\n",
      "[I 2026-01-07 18:43:46,027] Trial 2 finished with value: 0.12907025776803493 and parameters: {'n_layers': 3, 'hidden_size': 128, 'activation': 'elu', 'dropout': 0.22576323885255273, 'lr': 0.004187214182175137, 'weight_decay': 1.575681547543402e-05, 'optimizer': 'Adam', 'batch_size': 128}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:46,812] Trial 3 finished with value: 0.1653507485985756 and parameters: {'n_layers': 3, 'hidden_size': 128, 'activation': 'leaky_relu', 'dropout': 0.26813890232660026, 'lr': 0.0011383649595510955, 'weight_decay': 3.388178140547026e-05, 'optimizer': 'Adam', 'batch_size': 32}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:47,151] Trial 4 finished with value: 0.5537018448114395 and parameters: {'n_layers': 1, 'hidden_size': 32, 'activation': 'elu', 'dropout': 0.44121210031475033, 'lr': 0.00010386907322224736, 'weight_decay': 2.7804420678952543e-05, 'optimizer': 'Adam', 'batch_size': 32}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:47,164] Trial 5 pruned. \n",
      "[I 2026-01-07 18:43:47,176] Trial 6 pruned. \n",
      "[I 2026-01-07 18:43:47,209] Trial 7 pruned. \n",
      "[I 2026-01-07 18:43:47,230] Trial 8 pruned. \n",
      "[I 2026-01-07 18:43:47,252] Trial 9 pruned. \n",
      "[I 2026-01-07 18:43:47,632] Trial 10 finished with value: 0.15116159742077193 and parameters: {'n_layers': 4, 'hidden_size': 128, 'activation': 'relu', 'dropout': 0.31924234333375406, 'lr': 0.003106949495680857, 'weight_decay': 0.0009791230090166081, 'optimizer': 'RMSprop', 'batch_size': 128}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:48,002] Trial 11 finished with value: 0.25377127528190613 and parameters: {'n_layers': 4, 'hidden_size': 128, 'activation': 'relu', 'dropout': 0.3348571984880765, 'lr': 0.0033219777471304845, 'weight_decay': 0.0009190637280440534, 'optimizer': 'RMSprop', 'batch_size': 128}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:48,379] Trial 12 finished with value: 0.13802354037761688 and parameters: {'n_layers': 4, 'hidden_size': 128, 'activation': 'relu', 'dropout': 0.3490833732811772, 'lr': 0.002916259885773927, 'weight_decay': 1.3065775557818598e-06, 'optimizer': 'RMSprop', 'batch_size': 128}. Best is trial 2 with value: 0.12907025776803493.\n",
      "[I 2026-01-07 18:43:48,401] Trial 13 pruned. \n",
      "[I 2026-01-07 18:43:48,422] Trial 14 pruned. \n",
      "[I 2026-01-07 18:43:48,438] Trial 15 pruned. \n",
      "[I 2026-01-07 18:43:48,484] Trial 16 pruned. \n",
      "[I 2026-01-07 18:43:48,524] Trial 17 pruned. \n",
      "[I 2026-01-07 18:43:48,656] Trial 18 pruned. \n",
      "[I 2026-01-07 18:43:48,686] Trial 19 pruned. \n",
      "[I 2026-01-07 18:43:48,704] Trial 20 pruned. \n",
      "[I 2026-01-07 18:43:48,730] Trial 21 pruned. \n",
      "[I 2026-01-07 18:43:48,752] Trial 22 pruned. \n",
      "[I 2026-01-07 18:43:48,775] Trial 23 pruned. \n",
      "[I 2026-01-07 18:43:48,789] Trial 24 pruned. \n",
      "[I 2026-01-07 18:43:48,950] Trial 25 pruned. \n",
      "[I 2026-01-07 18:43:49,196] Trial 26 pruned. \n",
      "[I 2026-01-07 18:43:49,222] Trial 27 pruned. \n",
      "[I 2026-01-07 18:43:49,399] Trial 28 pruned. \n",
      "[I 2026-01-07 18:43:49,432] Trial 29 pruned. \n",
      "[I 2026-01-07 18:43:49,458] Trial 30 pruned. \n",
      "[I 2026-01-07 18:43:49,492] Trial 31 pruned. \n",
      "[I 2026-01-07 18:43:49,524] Trial 32 pruned. \n",
      "[I 2026-01-07 18:43:49,556] Trial 33 pruned. \n",
      "[I 2026-01-07 18:43:49,584] Trial 34 pruned. \n",
      "[I 2026-01-07 18:43:49,606] Trial 35 pruned. \n",
      "[I 2026-01-07 18:43:49,633] Trial 36 pruned. \n",
      "[I 2026-01-07 18:43:49,645] Trial 37 pruned. \n",
      "[I 2026-01-07 18:43:49,682] Trial 38 pruned. \n",
      "[I 2026-01-07 18:43:49,695] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BEST RESULT ================\n",
      "\n",
      "Best Validation Loss: 0.12907025776803493\n",
      "\n",
      "Best Hyperparameters:\n",
      "n_layers: 3\n",
      "hidden_size: 128\n",
      "activation: elu\n",
      "dropout: 0.22576323885255273\n",
      "lr: 0.004187214182175137\n",
      "weight_decay: 1.575681547543402e-05\n",
      "optimizer: Adam\n",
      "batch_size: 128\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Imports\n",
    "# =====================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "# =====================================================\n",
    "# Reproducibility\n",
    "# =====================================================\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# =====================================================\n",
    "# Dataset\n",
    "# =====================================================\n",
    "X, y = make_classification(\n",
    "    n_samples=1500,\n",
    "    n_features=10,\n",
    "    n_classes=2,\n",
    "    n_informative=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "# =====================================================\n",
    "# Dynamic Model Builder\n",
    "# =====================================================\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, hidden_size, activation, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_size))\n",
    "\n",
    "            if activation == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == \"tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == \"elu\":\n",
    "                layers.append(nn.ELU())\n",
    "            elif activation == \"leaky_relu\":\n",
    "                layers.append(nn.LeakyReLU(0.1))\n",
    "\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# =====================================================\n",
    "# Training & Validation\n",
    "# =====================================================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# =====================================================\n",
    "# Optuna Objective Function\n",
    "# =====================================================\n",
    "def objective(trial):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # Architecture hyperparameters\n",
    "    # ------------------------------\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16, 32, 64, 128])\n",
    "    activation = trial.suggest_categorical(\n",
    "        \"activation\", [\"relu\", \"tanh\", \"elu\", \"leaky_relu\"]\n",
    "    )\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Optimization hyperparameters\n",
    "    # ------------------------------\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\n",
    "        \"optimizer\", [\"Adam\", \"AdamW\", \"RMSprop\"]\n",
    "    )\n",
    "    batch_size = trial.suggest_categorical(\n",
    "        \"batch_size\", [32, 64, 128]\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Data loaders\n",
    "    # ------------------------------\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Model\n",
    "    # ------------------------------\n",
    "    model = DynamicNet(\n",
    "        input_dim=10,\n",
    "        n_layers=n_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        activation=activation,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Optimizer\n",
    "    # ------------------------------\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(\n",
    "            model.parameters(), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training loop with pruning\n",
    "    # ------------------------------\n",
    "    max_epochs = 25\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "        val_loss = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Report to Optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "\n",
    "        # Prune bad trials\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# =====================================================\n",
    "# Run Study\n",
    "# =====================================================\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "# =====================================================\n",
    "# Results\n",
    "# =====================================================\n",
    "print(\"\\n================ BEST RESULT ================\\n\")\n",
    "print(\"Best Validation Loss:\", study.best_value)\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40533da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
